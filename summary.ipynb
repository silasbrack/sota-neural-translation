{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summary.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "PGXMQZ53py3b"
      },
      "source": [
        "#@title Set Up\n",
        "%%capture\n",
        "!pip install torchtext==0.6.0 --quiet\n",
        "import gdown\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import random\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from pprint import pprint\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "!python -m spacy download en --quiet\n",
        "!python -m spacy download de --quiet\n",
        "spacy_german = spacy.load(\"de\")\n",
        "spacy_english = spacy.load(\"en\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def tokenize_german(text):\n",
        "  return [token.text for token in spacy_german.tokenizer(text)]\n",
        "\n",
        "def tokenize_english(text):\n",
        "  return [token.text for token in spacy_english.tokenizer(text)]\n",
        "german = Field(tokenize=tokenize_german,\n",
        "               lower=True,\n",
        "               init_token=\"<sos>\",\n",
        "               eos_token=\"<eos>\")\n",
        "\n",
        "english = Field(tokenize=tokenize_english,\n",
        "               lower=True,\n",
        "               init_token=\"<sos>\",\n",
        "               eos_token=\"<eos>\")\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = (\".de\", \".en\"),\n",
        "                                                    fields=(german, english))\n",
        "\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=3)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=3)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gqAW9q-frgMt"
      },
      "source": [
        "#@title LSTM\n",
        "%%capture\n",
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.tag = True\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    \n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "  # Shape of x (26, 32) [Sequence_length, batch_size]\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    \n",
        "    # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
        "\n",
        "    return hidden_state, cell_state\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  # Shape of x (32) [batch_size]\n",
        "  def forward(self, x, hidden_state, cell_state):\n",
        "\n",
        "    # Shape of x (1, 32) [1, batch_size]\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
        "\n",
        "    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n",
        "    predictions = self.fc(outputs)\n",
        "\n",
        "    # Shape --> predictions (32, 4556) [batch_size , output_size]\n",
        "    predictions = predictions.squeeze(0)\n",
        "\n",
        "    return predictions, hidden_state, cell_state\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.Encoder_LSTM = Encoder_LSTM\n",
        "    self.Decoder_LSTM = Decoder_LSTM\n",
        "\n",
        "  def forward(self, source, target, tfr=0.5):\n",
        "    # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n",
        "    batch_size = source.shape[1]\n",
        "\n",
        "    # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(english.vocab)\n",
        "    \n",
        "    # Shape --> outputs (14, 32, 5766) \n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n",
        "    hidden_state, cell_state = self.Encoder_LSTM(source)\n",
        "\n",
        "    # Shape of x (32 elements)\n",
        "    x = target[0] # Trigger token <SOS>\n",
        "\n",
        "    for i in range(1, target_len):\n",
        "      # Shape --> output (32, 5766) \n",
        "      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n",
        "      outputs[i] = output\n",
        "      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
        "      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
        "\n",
        "    # Shape --> outputs (14, 32, 5766) \n",
        "    return outputs\n",
        "\n",
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "    spacy_ger = spacy.load(\"de\")\n",
        "\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]\n",
        "input_size_encoder = len(german.vocab)\n",
        "encoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "encoder_dropout = 0.5\n",
        "\n",
        "encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
        "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
        "input_size_decoder = len(english.vocab)\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "decoder_dropout = 0.5\n",
        "output_size = len(english.vocab)\n",
        "decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n",
        "                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
        "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
        "!gdown --id 1-C6q-mBWYXMSFet9z-TtShfm10uiaRrI\n",
        "model.load_state_dict(torch.load('/content/checkpoint-NMT-SD3'))\n",
        "model.eval()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHb3bVL9qoS7"
      },
      "source": [
        "sentence = \"Nein, der alte Mann kommt nicht nach Hause.\"\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "DWiOTrYPutuD",
        "outputId": "386ba1ab-9b98-4595-87cd-3851a6958606"
      },
      "source": [
        "#@title Translate\n",
        "print(\" \".join(translate_sentence(model, sentence , german, english, device)[1:-1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man learning how to get his .\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}